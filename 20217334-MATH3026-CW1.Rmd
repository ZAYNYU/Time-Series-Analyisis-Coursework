---
title: "20217334-MATH3026-CW1"
author: "Zihang Yu"
date: "2023-3-10"
output:
  html_document:
    toc: no
  word_document:
    toc: no
---

# Q1(a)

<font size="4">
$\left\{X_t: t\in \mathbb{Z}\right\}$ is a ARMA(2,1) process
</font> 

# Q1(b)

<font size="4">

$\left\{X_t: t\in \mathbb{Z}\right\}$ is a (weakly) stationary process. <br> 

Since
<center>
$X_t = \frac{5}{6}X_{t-1} - \frac{1}{6}X_{t-2} +Z_t+\frac{1}{4}Z_{t-1}$  <br> 

$X_t - \frac{5}{6}BX_{t} + \frac{1}{6}B^2X_{t}  = Z_t+\frac{1}{4}BZ_{t}$  <br> 
</center>

Where B is the backward shift operator <br> 

<center>
$X_t(1 - \frac{5}{6}B + \frac{1}{6}B^2)  = Z_t(1+\frac{1}{4}B)$ <br> 
</center>

Thus,

<center>
$\phi(B) = (1 - \frac{5}{6}B + \frac{1}{6}B^2)$ <br> 
$\theta(B) = (1+\frac{1}{4}B)$ <br> 
</center>

Solve, 

<center>
\ $\phi(B) = (1 - \frac{5}{6}B + \frac{1}{6}B^2) = 0$ <br> 
</center>

We get,

<center>
$\frac{1}{6}(B^2-5B+6) = 0$ <br> 

$\frac{1}{6}(B-2)(B-3) = 0$ <br> 
</center>

Thus the roots of $\phi(B)$ \ are \ $B=2$ \ or \ $B=3$. Which are all lie outside the unit circle. <br>
From the lecture notes we know that if the roots of $\phi(B)$ lie outside the unit circle then the ARMA(2,1) process $\left\{X_t: t\in \mathbb{Z}\right\}$ is a (weakly) stationary process.

</font> 


# Q1(c)
<font size="4">

From Q1(b) we get,<br>

<center>
$\theta(B) = (1+\frac{1}{4}B)$ <br> 
</center>


From the lecture notes we know that the ARMA(2,1) process is invertable if the roots of $\theta(B) = (1+\frac{1}{4}B) = 0$ lie outside the unit circle. <br>

Solve, 

<center>
$\theta(B) = (1+\frac{1}{4}B) = 0$ <br>
</center>

We get, B = -4 which lie outside the unit circle, thus the ARMA(2,1) process $\left\{X_t: t\in \mathbb{Z}\right\}$ is invertable.

</font> 

# Q1(d)
<font size="4">

The $MA(\infty)$ form is $X_t = \sum_{j=0}^\infty \alpha_jZ_{t-j}$ <br>

Rearranging $X_t$ we get, <br>

<center>
$(1 - \frac{5}{6}B + \frac{1}{6}B^2)X_t  = (1+\frac{1}{4}B)Z_t$ <br>

$(\frac{1}{2}B-1)(\frac{1}{3}B-1)X_t = (1+\frac{1}{4}B)Z_t$ <br> 

$X_t = \frac{(1+\frac{1}{4}B)}{(1-\frac{1}{2}B)(1-\frac{1}{3}B)} Z_t$ <br> 
</center>

Let, <br>
<center>
$\frac{(1+\frac{1}{4}B)}{(1-\frac{1}{2}B)(1-\frac{1}{3}B)} Z_t = (\frac{A}{(1-\frac{1}{2}B)} + \frac{C}{(1-\frac{1}{3}B)}) Z_t$  <br> 
</center>
We get,  <br> 
<center>
$A = \frac{9}{2}, \ C = -\frac{7}{2}$ <br> 
</center>
Thus, <br> 
<center>
$X_t =  (\frac{\frac{9}{2}}{(1-\frac{1}{2}B)} - \frac{\frac{7}{2}}{(1-\frac{1}{3}B)}) Z_t$
</center>
Take Taylor expansions at 0: <br> 

<center>
$(1-\frac{1}{2}B)^{-1}$ = $\sum_{j=0}^\infty(\frac{1}{2})^jB^j$ <br> 

$(1-\frac{1}{3}B)^{-1}$ = $\sum_{j=0}^\infty(\frac{1}{3})^jB^j$ <br> 

</center>

Thus,<br>

<center>
$X_t = (\frac{9}{2}\sum_{j=0}^\infty(\frac{1}{2})^jB^j - \frac{7}{2}\sum_{j=0}^\infty(\frac{1}{3})^jB^j)Z_t$ <br>

$X_t = (\frac{9}{2}\sum_{j=0}^\infty(\frac{1}{2})^j - \frac{7}{2}\sum_{j=0}^\infty(\frac{1}{3})^j)Z_{t-j}$ <br>

$X_t = (\sum_{j=0}^\infty\frac{9}{2}(\frac{1}{2})^j - \frac{7}{2}(\frac{1}{3})^j)Z_{t-j}$ <br>

</center>

Above all, the process in $MA(\infty)$ form is: <br>
<center>

$X_t = \sum_{j=0}^\infty \alpha_jZ_{t-j}$ <br>
</center>
where,
<center>
$\alpha_j = \frac{9}{2}(\frac{1}{2})^j - \frac{7}{2}(\frac{1}{3})^j$ \ ,
</center>

</font> 

# Q1(e)
<font size="4">

<center>
$X_t = \frac{5}{6}X_{t-1} - \frac{1}{6}X_{t-2} +Z_t+\frac{1}{4}Z_{t-1}$ \ (1)  <br>
</center>

Multiplying both sides of (1) by $Z_t$ and taking expectations, <br>

<center>
$E(X_tZ_t) = \frac{5}{6}E(X_{t-1}Z_t) - \frac{1}{6}E(X_{t-2}Z_t) +E(Z_t^2)+\frac{1}{4}E(Z_{t-1}Z_t)$ <br>
</center>

By causality assumption $E(X_{t-1}Z_t) =0,\ E(X_{t-2}Z_t) = 0$ <br>
Since $Z_{t-1}$ and $Z_t$ are independent, $E(Z_{t-1}Z_t) = 0$ <br>

we get, <br>

<center>
$E(X_tZ_t) = \sigma_z^2$ \ (2)
</center>

Multiplying both sides of (1) by $X_t$ and taking expectations, <br>

<center>
$E(X_t^2) = \frac{5}{6}E(X_{t-1}X_t) - \frac{1}{6}E(X_{t-2}X_t) +E(Z_tX_t)+\frac{1}{4}E(Z_{t-1}X_t)$ <br>
</center>

We get, <br>

<center>
$\gamma(0) = \frac{5}{6}\gamma(1) - \frac{1}{6}\gamma(2) + \sigma_z^2 +\frac{1}{4}E(X_tZ_{t-1})$ \ (3)
</center>

Multiplying both sides of (1) by $Z_{t-1}$ and taking expectations, <br>

<center>
$E(X_tZ_{t-1}) = \frac{5}{6}E(X_{t-1}Z_{t-1}) - \frac{1}{6}E(X_{t-2}Z_{t-1}) +E(Z_tZ_{t-1})+\frac{1}{4}E(Z_{t-1}^2)$ <br>
</center>

Thus, <br>

<center>
$E(Z_{t-1}X_t) = \frac{5}{6}\sigma_z^2 + \frac{1}{4}\sigma_z^2 = \frac{13}{12}\sigma_z^2$ \ (4) <br>
</center>

Substitude (4) into (3) we get, <br>

<center>
$\gamma(0) = \frac{5}{6}\gamma(1) - \frac{1}{6}\gamma(2) + \frac{61}{48} \sigma_z^2$ \ (5) <br>
</center>

Multiplying both sides of (1) by $X_{t-1}$ and taking expectations, <br>

<center>
$E(X_tX_{t-1}) = \frac{5}{6}E(X_{t-1}X_{t-1}) - \frac{1}{6}E(X_{t-2}X_{t-1}) +E(Z_tX_{t-1})+\frac{1}{4}E(Z_{t-1}X_{t-1})$ <br>
</center>

We get, <br>

<center>
$\gamma(1) = \frac{5}{6}\gamma(0) - \frac{1}{6}\gamma(1) + \frac{1}{4}\sigma_z^2$ \ (6) <br>
</center>

Multiplying both sides of (1) by $X_{t-2}$ and taking expectations, <br>

<center>
$E(X_tX_{t-2}) = \frac{5}{6}E(X_{t-1}X_{t-2}) - \frac{1}{6}E(X_{t-2}^2) +E(Z_tX_{t-2})+\frac{1}{4}E(Z_{t-1}X_{t-2})$ <br>
</center>

We get, <br>

<center>
$\gamma(2) = \frac{5}{6}\gamma(1) - \frac{1}{6}\gamma(0)$ \ (7)
</center>

By solving (5), (6), (7) <br>
<center>
$\gamma(0) =  \frac{477}{160}\sigma_z^2$

$\gamma(1) =  \frac{75}{32}\sigma_z^2$
</center>

Multiplying both sides of (1) by $X_{t-h}$ and taking expectations, <br>

<center>
$E(X_tX_{t-h}) = \frac{5}{6}E(X_{t-1}X_{t-h}) - \frac{1}{6}E(X_{t-2}X_{t-h}) +E(Z_tX_{t-h})+\frac{1}{4}E(Z_{t-1}X_{t-h})$ <br>
</center>

We get, <br>

<center>
$\gamma(h) = \frac{5}{6}\gamma(h-1) - \frac{1}{6}\gamma(h-2)$ \ (8)
</center>

The auxiliary equation is, <br>
<center>
$\lambda^2-\frac{5}{6}\lambda+\frac{1}{6} = 0$ <br>
$(\lambda-\frac{1}{2})(\lambda-\frac{1}{3}) = 0$ <br>
</center>
roots are $λ_1 = \frac{1}{2}$ , and $λ_2 = \frac{1}{2}$ <br>
Thus the general solution is of the form: <br>

<center>
$\gamma(h) = C_1(\frac{1}{2})^{|h|} + C_2(\frac{1}{3})^{|h|}$ <br>
</center>

To find $C_1$ and $C_2$, <br>

When h = 0, <br>

<center>
$\gamma(0) = C_1 + C_2 = \frac{477}{160}\sigma_z^2$ <br>
</center>

When h = 1, <br>

<center>
$\gamma(1) = C_1(\frac{1}{2}) + C_2(\frac{1}{3}) = \frac{75}{32}\sigma_z^2$ <br>
</center>

Solve for $C_1, C_2$, we get,

<center>

$C_1 = \frac{81}{10}\sigma_z^2 , \ C_2 = -\frac{819}{160}\sigma_z^2$ <br>

</center>

Above all, <br>

The autocovariance function for the process is, <br>

<center>
$\gamma(h) = (\frac{81}{10}(\frac{1}{2})^{|h|} -\frac{819}{160}(\frac{1}{3})^{|h|}) \sigma_z^2$
</center>
</font>

\newpage

# Q2

<font size="4">
For all the plots in Q2, the red lines are theoretical ACF/PACF.<br>
</font>

## Case 1, n = 20

<font size="4">
The plot of theoretical and sample ACF and PACF when simulating data from the given process of sample size n = 20.
</font>

```{r,echo=FALSE}
#Set the seed
set.seed(7334)
#Simulate 20 observations from the ARMA(3,2) process with
#phi1 = -0.8, phi2 = 0.4, phi3 = 0.3, theta1 = 0.75, theta2 = -0.6 and sigma_z = 1
x1 <- arima.sim(n=20,model=list(ar=c(-0.8,0.4,0.3),ma=c(0.75,-0.6)), sd = 1)

#Simulate 100 observations from the process above 
x2 <- arima.sim(n=100,model=list(ar=c(-0.8,0.4,0.3),ma=c(0.75,-0.6)), sd = 1)

#Simulate 1000 observations from the process above 
x3 <- arima.sim(n=1000,model=list(ar=c(-0.8,0.4,0.3),ma=c(0.75,-0.6)), sd = 1)

#Simulate 10000 observations from the process above 
x4 <- arima.sim(n=10000,model=list(ar=c(-0.8,0.4,0.3),ma=c(0.75,-0.6)), sd = 1)

#Calculate the theoretical ACF and PACF for the process above with lags up to 10
t_acf1 <- ARMAacf(ar=c(-0.8,0.4,0.3),ma=c(0.75,-0.6),lag.max=10)
t_pacf1 <- ARMAacf(ar=c(-0.8,0.4,0.3),ma=c(0.75,-0.6),lag.max=10, pacf=TRUE)

#Calculate the theoretical ACF and PACF for the process above with lags up to 25
t_acf2 <- ARMAacf(ar=c(-0.8,0.4,0.3),ma=c(0.75,-0.6),lag.max=25)
t_pacf2 <- ARMAacf(ar=c(-0.8,0.4,0.3),ma=c(0.75,-0.6),lag.max=25, pacf=TRUE)

#Set the two plots with 1 row and 2 columns
par(mfrow=c(1,2))

#Plot the sample ACF of x1
acf(x1,xlim=c(0,10))
#Add the theoretical ACF to this plot as a red line
lines(c(0:10),t_acf1,col="red")

#Plot the sample PACF of x1
#ylim=c(-1,1) ensures that the vertical axis runs from -1 to 1.
pacf(x1,xlim = c(1,10),ylim=c(-1,1))
#Add the theoretical PACF to this plot as a red line
lines(c(1:10),t_pacf1,col="red")
```

<font size="4">
To measure the theoretical and sample ACF and PACF numerically, we can calculate the mean square error <br>
which is: <br>
$mean \ square \ error \ ACF = \frac{1}{n+1}\sum(theoretical \  ACF - sample \ ACF)^2$ <br>
$mean \ square \ error \ PACF = \frac{1}{n}\sum(theoretical \  PACF - sample \ PACF)^2$ <br>
Where n is the maximum lags.
</font>
 
```{r,echo = FALSE, results='hide',  include=FALSE}
#Calculate the sample acf of x1 with maximum lag 10
y_acf1 <- acf(x1,lag.max = 10)

# store the sample acf as a vector
y_sample_acf1 <- as.vector(y_acf1$acf[,,1])

#Calculate the sample pacf of x1 with maximum lag 10
y_pacf1 <- pacf(x1,lag.max = 10)

# store the sample pacf as a vector
y_sample_pacf1 <- as.vector(y_pacf1$acf[,,1])

```

<font size="4">
In this case, the values of the sample ACF when simulating data from the given process of sample size n = 20, with maximum lag 10 are,

```{r, echo=FALSE}
y_sample_acf1
```

The theoretical ACF values are,

```{r, echo=FALSE}
unname(t_acf1)
#theoretical ACF values
```
</font>

<font size="4">
Hence, the mean square error for ACF is,
</font>
```{r, echo=FALSE}
(sum((y_sample_acf1 - t_acf1)^2))/11
```

<font size="4">
The values of the sample PACF when simulating data from the given process of sample size n = 20, with maximum lag 10 are,

```{r, echo=FALSE}
y_sample_pacf1
```

The theoretical PACF values are,

```{r, echo=FALSE}
t_pacf1
#theoretical PACF values
```
</font>

<font size="4">
Hence, the mean square error for PACF is,
</font>
```{r, echo=FALSE}
(sum((y_sample_pacf1 - t_pacf1)^2))/10
# calculate the sum of the square of the errors
```

## Case 2, n = 100

<font size="4">
The plot of theoretical and sample ACF and PACF when simulating data from the given process of sample size n = 100.
</font>

```{r,echo=FALSE}
#Set the two plots with 1 row and 2 columns
par(mfrow=c(1,2))

#Plot the sample ACF of x2
acf(x2,xlim=c(0,25))
#Add the theoretical ACF to this plot as a red line
lines(c(0:25),t_acf2,col="red")

#Plot the sample PACF of x2
#ylim=c(-1,1) ensures that the vertical axis runs from -1 to 1.
pacf(x2,xlim=c(1,25), ylim=c(-1,1))
#Add the theoretical PACF to this plot as a red line
lines(c(1:25),t_pacf2,col="red")
```

```{r,echo = FALSE, results='hide',  include=FALSE}
#Calculate the sample acf of x2 with maximum lag 25
y_acf2 <- acf(x2,lag.max = 25)

# store the sample acf as a vector
y_sample_acf2 <- as.vector(y_acf2$acf[,,1])

#Calculate the sample pacf of x2 with maximum lag 25
y_pacf2 <- pacf(x2,lag.max = 25)

# store the sample pacf as a vector
y_sample_pacf2 <- as.vector(y_pacf2$acf[,,1])
```


<font size="4">
Same as above, to measure the theoretical and sample ACF and PACF numerically, we can calculate the mean square errors <br>
</font>

<font size="4">
The mean square error for ACF is,
</font>

```{r, echo=FALSE}
(sum((y_sample_acf2 - t_acf2)^2))/26
# calculate the sum of the square of the errors
```

<font size="4">
The mean square error for PACF is,
</font>
```{r, echo=FALSE}
(sum((y_sample_pacf2 - t_pacf2)^2))/25
# calculate the sum of the square of the errors
```


## Case 3, n = 1000

<font size="4">
The plot of theoretical and sample ACF and PACF when simulating data from the given process of sample size n = 1000.
</font>

```{r,echo=FALSE}
#Set the two plots with 1 row and 2 columns
par(mfrow=c(1,2))

#Plot the sample ACF of x3
acf(x3,xlim=c(0,25))
#Add the theoretical ACF to this plot as a red line
lines(c(0:25),t_acf2,col="red")

#Plot the sample PACF of x3
#ylim=c(-1,1) ensures that the vertical axis runs from -1 to 1.
pacf(x3,xlim=c(1,25),ylim=c(-1,1))
#Add the theoretical PACF to this plot as a red line
lines(c(1:25),t_pacf2,col="red")
```


```{r,echo = FALSE, results='hide',  include=FALSE}
#Calculate the sample acf of x3 with maximum lag 25
y_acf3 <- acf(x3,lag.max = 25)

# store the sample acf as a vector
y_sample_acf3 <- as.vector(y_acf3$acf[,,1])

#Calculate the sample pacf of x3 with maximum lag 25
y_pacf3 <- pacf(x3,lag.max = 25)

# store the sample pacf as a vector
y_sample_pacf3 <- as.vector(y_pacf3$acf[,,1])
```


<font size="4">
Same as above, to measure the theoretical and sample ACF and PACF numerically, we can calculate the mean square errors <br>
</font>

<font size="4">
The mean square error for ACF is,
</font>

```{r, echo=FALSE}
(sum((y_sample_acf3 - t_acf2)^2))/26
# calculate the sum of the square of the errors
```

<font size="4">
The mean square error for PACF is,
</font>
```{r, echo=FALSE}
(sum((y_sample_pacf3 - t_pacf2)^2))/25
# calculate the sum of the square of the errors
```



## Case 4, n = 10000
<font size="4">
The plot of theoretical and sample ACF and PACF when simulating data from the given process of sample size n = 10000.
</font>

```{r,echo=FALSE}
#Set the two plots with 1 row and 2 columns
par(mfrow=c(1,2))

#Plot the sample ACF of x4
acf(x4,xlim=c(0,25))
#Add the theoretical ACF to this plot as a red line
lines(c(0:25),t_acf2,col="red")

#Plot the sample PACF of x4
#ylim=c(-1,1) ensures that the vertical axis runs from -1 to 1.
pacf(x4,xlim=c(1,25),ylim=c(-1,1))
#Add the theoretical PACF to this plot as a red line
lines(c(1:25),t_pacf2,col="red")
```

```{r,echo = FALSE, results='hide',  include=FALSE}
#Calculate the sample acf of x4 with maximum lag 25
y_acf4 <- acf(x4,lag.max = 25)

# store the sample acf as a vector
y_sample_acf4 <- as.vector(y_acf4$acf[,,1])

#Calculate the sample pacf of x4 with maximum lag 25
y_pacf4 <- pacf(x4,lag.max = 25)

# store the sample pacf as a vector
y_sample_pacf4 <- as.vector(y_pacf4$acf[,,1])
```


<font size="4">
Same as above, to measure the theoretical and sample ACF and PACF numerically, we can calculate the mean square errors <br>
</font>

<font size="4">
The mean square error for ACF is,
</font>

```{r, echo=FALSE}
(sum((y_sample_acf4 - t_acf2)^2))/26
# calculate the sum of the square of the errors
```

<font size="4">
The mean square error for PACF is,
</font>
```{r, echo=FALSE}
(sum((y_sample_pacf4 - t_pacf2)^2))/25
# calculate the sum of the square of the errors
```


## Comments: 
<font size="4">
From the plots above, <br>

When n = 20, the sample size is small, thus the sample ACF/PACF are more variable and may not provide a reliable estimate of the theoretical ACF/PACF. <br>

When n = 100, the sample ACF/PACF provide a better estimate of the theoretical ACF/PACF. However, there may still be some variability in the sample ACF/PACF. <br>

When n = 1000, With a larger sample size of 1000, the sample ACF/PACF converge to the theoretical ACF/PACF more closely. The sample ACF/PACF provide a more reliable estimate of the properties of the process. <br>

When n = 10000, the sample ACF/PACF converge even more closely to the theoretical ACF/PACF. The sample ACF/PACF should provide a highly reliable estimate of the properties of the given process. <br>

In general, it is obvious that the peaks of the red lines are getting dense, and the theoretical and the sample ACF/PACF are getting closer to each other as the simulated sample size n increases. <br>

Numerically, the result of mean square errors for the theoretical and the sample ACF/PACF are getting smaller as the simulated samples size increased.

```{r,echo=FALSE,message=FALSE, warning=FALSE}
df <- data.frame(
  sample_size = c("20", "100", "1000" , "10000"),
  MSE_ACF = c(0.04201092, 0.01708503, 0.00144035, 7.279064e-05),
  MSE_PACF = c(0.03087296, 0.008975989, 0.0006801876, 9.928432e-05)
)
library(knitr)
kable(df, caption = "Mean square errors for different sample size")
```

Therefore, we could conclude that the larger the sample size is, the quicker the sample ACF and PACF converge to their theoretical values. 
</font>

\pagebreak

```{r, eval=FALSE}
# Appendix with R code

#Set the seed
set.seed(7334)
#Simulate 20 observations from the ARMA(3,2) process with
#phi1 = -0.8, phi2 = 0.4, phi3 = 0.3, theta1 = 0.75, theta2 = -0.6 and sigma_z = 1
x1 <- arima.sim(n=20,model=list(ar=c(-0.8,0.4,0.3),ma=c(0.75,-0.6)), sd = 1)

#Simulate 100 observations from the process above 
x2 <- arima.sim(n=100,model=list(ar=c(-0.8,0.4,0.3),ma=c(0.75,-0.6)), sd = 1)

#Simulate 1000 observations from the process above 
x3 <- arima.sim(n=1000,model=list(ar=c(-0.8,0.4,0.3),ma=c(0.75,-0.6)), sd = 1)

#Simulate 10000 observations from the process above 
x4 <- arima.sim(n=10000,model=list(ar=c(-0.8,0.4,0.3),ma=c(0.75,-0.6)), sd = 1)

#Calculate the theoretical ACF and PACF for the process above with lags up to 10
t_acf1 <- ARMAacf(ar=c(-0.8,0.4,0.3),ma=c(0.75,-0.6),lag.max=10)
t_pacf1 <- ARMAacf(ar=c(-0.8,0.4,0.3),ma=c(0.75,-0.6),lag.max=10, pacf=TRUE)

#Calculate the theoretical ACF and PACF for the process above with lags up to 25
t_acf2 <- ARMAacf(ar=c(-0.8,0.4,0.3),ma=c(0.75,-0.6),lag.max=25)
t_pacf2 <- ARMAacf(ar=c(-0.8,0.4,0.3),ma=c(0.75,-0.6),lag.max=25, pacf=TRUE)

#Set the two plots with 1 row and 2 columns
par(mfrow=c(1,2))

#Plot the sample ACF of x1
acf(x1,xlim=c(0,10))
#Add the theoretical ACF to this plot as a red line
lines(c(0:10),t_acf1,col="red")

#Plot the sample PACF of x1
#ylim=c(-1,1) ensures that the vertical axis runs from -1 to 1.
pacf(x1,xlim=c(1,10),ylim=c(-1,1))
#Add the theoretical PACF to this plot as a red line
lines(c(1:10),t_pacf1,col="red")
#Calculate the sample acf of x1 with maximum lag 10
y_acf1 <- acf(x1,lag.max = 10)

# store the sample acf as a vector
y_sample_acf1 <- as.vector(y_acf1$acf[,,1])

#Calculate the sample pacf of x1 with maximum lag 10
y_pacf1 <- pacf(x1,lag.max = 10)

# store the sample pacf as a vector
y_sample_pacf1 <- as.vector(y_pacf1$acf[,,1])

unname(t_acf1)
#theoretical ACF values

y_sample_acf1
(sum((y_sample_acf1 - t_acf1)^2))/11
# calculate the sum of the square of the errors

t_pacf1
#theoretical PACF values

y_sample_pacf1
(sum((y_sample_pacf1 - t_pacf1)^2))/10
# calculate the sum of the square of the errors

#Set the two plots with 1 row and 2 columns
par(mfrow=c(1,2))

#Plot the sample ACF of x2
acf(x2,xlim=c(0,25))
#Add the theoretical ACF to this plot as a red line
lines(c(0:25),t_acf2,col="red")

#Plot the sample PACF of x2
#ylim=c(-1,1) ensures that the vertical axis runs from -1 to 1.
pacf(x2,xlim=c(1,25),ylim=c(-1,1))
#Add the theoretical PACF to this plot as a red line
lines(c(1:25),t_pacf2,col="red")

#Calculate the sample acf of x2 with maximum lag 25
y_acf2 <- acf(x2,lag.max = 25)

# store the sample acf as a vector
y_sample_acf2 <- as.vector(y_acf2$acf[,,1])

#Calculate the sample pacf of x2 with maximum lag 25
y_pacf2 <- pacf(x2,lag.max = 25)

# store the sample pacf as a vector
y_sample_pacf2 <- as.vector(y_pacf2$acf[,,1])

(sum((y_sample_acf2 - t_acf2)^2))/26
# calculate the sum of the square of the errors


(sum((y_sample_pacf2 - t_pacf2)^2))/25
# calculate the sum of the square of the errors

#Set the two plots with 1 row and 2 columns
par(mfrow=c(1,2))

#Plot the sample ACF of x3
acf(x3,xlim=c(0,25))
#Add the theoretical ACF to this plot as a red line
lines(c(0:25),t_acf2,col="red")

#Plot the sample PACF of x3
#ylim=c(-1,1) ensures that the vertical axis runs from -1 to 1.
pacf(x3,xlim=c(1,25),ylim=c(-1,1))
#Add the theoretical PACF to this plot as a red line
lines(c(1:25),t_pacf2,col="red")

#Calculate the sample acf of x3 with maximum lag 25
y_acf3 <- acf(x3,lag.max = 25)

# store the sample acf as a vector
y_sample_acf3 <- as.vector(y_acf3$acf[,,1])

#Calculate the sample pacf of x3 with maximum lag 25
y_pacf3 <- pacf(x3,lag.max = 25)

# store the sample pacf as a vector
y_sample_pacf3 <- as.vector(y_pacf3$acf[,,1])

(sum((y_sample_acf2 - t_acf2)^2))/26
# calculate the sum of the square of the errors
(sum((y_sample_pacf2 - t_pacf2)^2))/25

#Set the two plots with 1 row and 2 columns
par(mfrow=c(1,2))

#Plot the sample ACF of x4
acf(x4,xlim=c(0,25))
#Add the theoretical ACF to this plot as a red line
lines(c(0:25),t_acf2,col="red")

#Plot the sample PACF of x4
#ylim=c(-1,1) ensures that the vertical axis runs from -1 to 1.
pacf(x4,xlim=c(1,25),ylim=c(-1,1))
#Add the theoretical PACF to this plot as a red line
lines(c(1:25),t_pacf2,col="red")


#Calculate the sample acf of x4 with maximum lag 25
y_acf4 <- acf(x4,lag.max = 25)

# store the sample acf as a vector
y_sample_acf4 <- as.vector(y_acf4$acf[,,1])

#Calculate the sample pacf of x4 with maximum lag 25
y_pacf4 <- pacf(x4,lag.max = 25)

# store the sample pacf as a vector
y_sample_pacf4 <- as.vector(y_pacf4$acf[,,1])

(sum((y_sample_acf4 - t_acf2)^2))/26
# calculate the sum of the square of the errors

(sum((y_sample_pacf4 - t_pacf2)^2))/25
# calculate the sum of the square of the errors

df <- data.frame(
  sample_size = c("20", "100", "1000" , "10000"),
  MSE_ACF = c(0.04201092, 0.01708503, 0.00144035, 7.279064e-05),
  MSE_PACF = c(0.03087296, 0.008975989, 0.0006801876, 9.928432e-05)
)
library(knitr)
kable(df, caption = "Mean square errors for different sample size")
```


















